---
title: "Step 1 - Filtering and processing location data"
format: html
editor: source
editor_options: 
  chunk_output_type: console
---

[ARG: Any underlined text in this document that start with ARG: represents notes to self on things that I need to further clarify and set up.]{.underline}

# Folder and file preparation

-   At minimum the working directory must contain a folder titled as follows:

raw_data

-   Within the raw_data folder there should be a folder titled "indiv_tags" that contains all the raw source files for the tags of interest (i.e., all the files downloaded from Wildlife Computers) as well as a CSV file titled "Metadata" containing the data for all the tags.

Also easier to have qmd files in the directory

[ARG: We need to provide a header template for the Metadata file to ensure harmony with ongoing data processing steps.]{.underline}

# [Step 1 - Initial data processing]{.underline}

### Install and load packages

The following chunk of code will check for the necessary packages (listed in `packages`) and if they are not installed on the users computer, will install them. They will then be loaded into the session. The `crawlUtils` package will be installed for every new session because it is under development.

```{r, output=FALSE}
# Package names
packages <- c("tidyverse","mapview","ggspatial","trip","crsuggest","units",
              "plotly","sf","janitor","lubridate","crsuggest","doFuture","picMaps","rosm", "terra")

# R package repositories
repos <- c(
  dsjohnson = 'https://dsjohnson.r-universe.dev',
  pifsc = 'https://pifsc-protected-species-division.r-universe.dev',
  jmlondon = 'https://jmlondon.r-universe.dev',
  CRAN = 'https://cloud.r-project.org'
)

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
install.packages(c(packages[!installed_packages], "crawlUtils"), 
                 repos=repos, dependencies=TRUE)
packages <- c(packages,"crawlUtils")

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
write_csv(data.frame(packages=packages), "packages.csv")
```

### Read in telemetry data, filter locations by Argos location class, and remove grossly inaccurate positions

In the code chunk below we are removing only "Z" location classes, but can remove others as well (e.g., LC "0", "B", "A"). For the third step, the code keeps only locations within the latitudes and longitudes provided. One way to determine this is to use Google Maps and click on locations near your study site and identify nearby and appropriate lat and long for this filter.

```{r}
locs <- cu_read_wc_dirs("raw_data/indiv_tags") %>% 
  filter(quality!="Z") %>% 
  filter(longitude>94, longitude<161, latitude>-9, latitude<29) 

```

### Filter out locations with comments

This filter was a quick and easy way to eliminate any location lines in the source file that has non-empty comments, with the idea that there might be some issues with those location lines. This is probably overkill and a course way to do this (you will lose some data that may be valid), but they represent \< 2% of all locations, so not considered a huge deal.

```{r}
locs <- filter(locs, is.na(comment))
```

### (Optional) Filter for GPS or Argos locations only

You can keep only FastGPS locations or only Argos locations by placing the following code in an R chunk, respectively:

```{r,eval=FALSE}
# locs <- locs %>% filter(type=="FastGPS")
```

or

```{r, eval=FALSE}
# locs <- locs %>% filter(type=="Argos")
```

### Convert location data to simple feature (sf) points

Convert data to R spatial object

```{r}
locs <- st_as_sf(locs, coords=c('longitude','latitude'), crs=4326)
```

### Import Metadata file

Make sure metadata CSV file is simply called "Metadata". This chunk also cleans any potential issues with the names (e.g., spaces and special characters), then renames the deploy ID provided in the Wildlife Computers output so it is called "deploy_id". The mutate code then ensures the deploy dates are of the appropriate format for processing.

### (Optional) Filter Metadata by project

If your metadata has multiple projects (in the "Project" column, you can assign them here (you will need to specify the project name in the quotation marks, below it is `"NAVFAC_NavyBaseGuam"`). If only one project this line can be skipped/eliminated.

```{r, output=FALSE}
meta_data <- read_csv("raw_data/Metadata.csv") %>% 
  clean_names() %>% 
  #indicate project here or eliminate this line
  #filter(project=="NAVFAC_NavyBaseGuam") %>% 
  rename(deploy_id = ptt_unique_id) %>% 
  mutate(deploy_date = mdy(deploy_date))
```

Next, filter out all telemetry locations for animals *not* in the meta-data

```{r}
locs <- filter(locs, deploy_id %in% meta_data$deploy_id)
```

### Remove locations prior to deployment date

Remove locations within, say, 24 hours of deployment to eliminate possible capture effects on movement. In the code below, `24` can be changed to whatever number of hours is desired.

```{r}
locs <- meta_data  %>% left_join(locs,.)
locs <- filter(locs, datetime >= (deploy_date+hours(24)) )
```

### Add crawl columns

Add columns for fitting CTCRW models to all data simultaneously

```{r}
locs <- cu_add_argos_cols(locs) 
```

### Remove duplicated locations

Remove duplicate locations (i.e., same date and time)

```{r}
locs <- locs %>% group_by(deploy_id) %>% arrange(datetime, error_area) %>%
  filter(!duplicated(datetime, fromLast=TRUE)) %>% ungroup() %>%
  arrange(deploy_id, datetime)

```

### Run sda speed filter

Here you can adjust the max speed between locations to remove biologically unreasonable results (i.e., the distance between three locations isn't realistic because a turtle can't move that fast. In code chunk below, the max speed =7.2 km/h = 2 m/s.

[ARG: Turn angle is incorporated in this code (need to clarify default angle) Speed, distance, angle ...in trip package]{.underline}

See [Freitas et al. (2008)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1748-7692.2007.00180.x?casa_token=AP2D16sj4I4AAAAA%3AEXLY23QX0WdBHr6xzU2IEO603U0beNMPDndJn-VXAYB4sm3QWkUvHTJbWqdJv0K7oD6QSolnM78eSgczI2Q) and `?trip::sda`.

```{r}
locs$keep <-locs %>%
  trip(c("datetime","deploy_id"),correct_all=FALSE) %>%
  sda(smax=7.2) 

locs <- filter(locs, keep)
```

### Get number of locations per animal

```{r}
anim_locs <- st_drop_geometry(locs) %>% select(deploy_id) %>% 
  group_by(deploy_id) %>% summarize(num_locs=n())
```

### Remove PTTs with \< a certain number of locations:

In this case it is tags with less than 20 locations, but that can be adjusted here.

```{r}
anim_locs <- filter(anim_locs, num_locs>20)
locs <- filter(locs, deploy_id %in% anim_locs$deploy_id)

```

### Project data for CTCRW modeling

[ARG: Devin to review the subsequent code for finding the appropriate projection (for area of interest)]{.underline}

Use `crsuggest` package to determine top projection. We will then project the data when we subset into segments. 

```{r}
prj <- as.numeric(suggest_crs(locs)$crs_code[[1]])
locs <- locs %>% st_transform(prj)
```


### Save output for continuing later

If you would would like to pause here and continue working later you should save the products so you don't have to repeat these steps.

```{r}
dir.create("processed_data")
save(locs, file="processed_data/clean_data.RData")
```

Otherwise, you can continue with step 2 without saving.

## Continue data analysis

Continue into step 2 directly. If R is closed prior to Step 2, Step 1 will need to be run again.
