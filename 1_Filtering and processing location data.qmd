---
title: "Step 1 - Filtering and processing location data"
format: html
editor: source
editor_options: 
  chunk_output_type: console
---

[ARG: Any underlined text in this document that start with ARG: represents notes to self on things that I need to further clarify and set up.]{.underline}

[ARG: I added the load_libraries.R file]{.underline} [to the parent folder, but need Devin to confirm all the necessary packages are listed.]{.underline}

# Folder and file preparation

-   The default working directory for Quarto is the directory of the Quarto Rmd file, so if you are storing this Quarto Rmd file in a different folder than the working director, you'll need to tell Quarto. To do so, go to: `Tools -> Global Options -> R Markdown` and choose the option - Evaluate chunks in "Current" directory.

-   At minimum the working directory must contain a folder titled as follows:

1.  raw_data

-   Within the raw_data folder there should be a folder titled "indiv_tags" that contains all the raw source files for the tags of interest (i.e., all the files downloaded from Wildlife Computers) as well as a CSV file titled "Metadata" containing the data for all the tags.

2.  load_libraries.R

-   Should contain all the packages needed for any of the processing in this sea turtle spatial analysis R processing code.

[ARG: We need to provide a header template for the Metadata file to ensure harmony with ongoing data processing steps.]{.underline}

# [Step 1 - Initial data processing]{.underline}

### Install and load packages

The following chunk of code will check for the necessary packages (listed in `packages`) and if they are not installed on the users computer, will install them. They will then be loaded into the session. The `crawlUtils` package will be installed for every new session because it is under development.

```{r, output=FALSE}
# Package names
packages <- c("tidyverse","mapview","ggspatial","trip","crsuggest","units",
              "plotly","sf", "janitor","lubridate","crsuggest","doFuture","picMaps")

# R package repositories
repos <- c(
  dsjohnson = 'https://dsjohnson.r-universe.dev',
  pifsc = 'https://pifsc-protected-species-division.r-universe.dev',
  jmlondon = 'https://jmlondon.r-universe.dev',
  CRAN = 'https://cloud.r-project.org'
)

# Install packages not yet installed
installed_packages <- packages %in% rownames(installed.packages())
install.packages(c(packages[!installed_packages], "crawlUtils"), 
                 repos=repos, dependencies=TRUE)
packages <- c(packages,"crawlUtils")

# Packages loading
invisible(lapply(packages, library, character.only = TRUE))
```

### Read in telemetry data, filter locations by Argos location class, and remove grossly inaccurate positions

In the code chunk below we are removing only "Z" location classes, but can remove others as well (e.g., LC "0", "B", "A"). For the third step, the code keeps only locations within the latitudes and longitudes provided. One way to determine this is to use Google Maps and click on locations near your study site and identify nearby and appropriate lat and long for this filter.

```{r}
locs <- cu_read_wc_dirs("raw_data/indiv_tags") %>% 
  filter(quality!="Z") %>% 
  filter(longitude>144, longitude<146, latitude>13, latitude<16) 

```

### Filter out locations with comments

This filter was a quick and easy way to eliminate any location lines in the source file that has non-empty comments, with the idea that there might be some issues with those location lines. This is probably overkill and a course way to do this (you will lose some data that may be valid), but they represent \< 2% of all locations, so not considered a huge deal.

```{r}
locs <- filter(locs, is.na(comment))
```

### (Optional) Filter for GPS or Argos locations only

You can keep only FastGPS locations or only Argos locations by placing the following code in an R chunk, respectively:

```{r,eval=FALSE}
# locs <- locs %>% filter(type=="FastGPS")
```

or

```{r, eval=FALSE}
# locs <- locs %>% filter(type=="Argos")
```

### Convert location data to simple feature (sf) points

Convert data to R spatial object

```{r}
locs <- st_as_sf(locs, coords=c('longitude','latitude'), crs=4326)
```

### Import Metadata file

Make sure metadata CSV file is simply called "Metadata". This chunk also cleans any potential issues with the names (e.g., spaces and special characters), then renames the deploy ID provided in the Wildlife Computers output so it is called "deploy_id". The mutate code then ensures the deploy dates are of the appropriate format for processing.

### (Optional) Filter Metadata by project

If your metadata has multiple projects (in the "Project" column, you can assign them here (you will need to specify the project name in the quotation marks, below it is `"NAVFAC_NavyBaseGuam"`). If only one project this line can be skipped/eliminated.

```{r, output=FALSE}
meta_data <- read_csv("raw_data/Metadata.csv") %>% 
  clean_names() %>% 
  #indicate project here or eliminate this line
  filter(project=="NAVFAC_NavyBaseGuam") %>% 
  rename(deploy_id = ptt_unique_id) %>% 
  mutate(deploy_date = mdy(deploy_date))
```

Next, filter out all telemetry locations for animals *not* in the meta-data

```{r}
locs <- filter(locs, deploy_id %in% meta_data$deploy_id)
```

### Remove locations prior to deployment date

Remove locations within, say, 24 hours of deployment to eliminate possible capture effects on movement. In the code below, `24` can be changed to whatever number of hours is desired.

```{r}
locs <- meta_data  %>% left_join(locs,.)
locs <- filter(locs, datetime >= (deploy_date+hours(24)) )
```

### Add crawl columns

Add columns for fitting CTCRW models to all data simultaneously

```{r}
locs <- cu_add_argos_cols(locs) 
```

### Remove duplicated locations

Remove duplicate locations (i.e., same date and time)

```{r}
locs <- locs %>% group_by(deploy_id) %>% arrange(datetime, error_area) %>%
  filter(!duplicated(datetime, fromLast=TRUE)) %>% ungroup() %>%
  arrange(deploy_id, datetime)

```

### Run sda speed filter

Here you can adjust the max speed between locations to remove biologically unreasonable results (i.e., the distance between three locations isn't realistic because a turtle can't move that fast. In code chunk below, the max speed =7.2 km/h = 2 m/s.

[ARG: Turn angle is incorporated in this code (need to clarify default angle) Speed, distance, angle ...in trip package]{.underline}

See [Freitas et al. (2008)](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1748-7692.2007.00180.x?casa_token=AP2D16sj4I4AAAAA%3AEXLY23QX0WdBHr6xzU2IEO603U0beNMPDndJn-VXAYB4sm3QWkUvHTJbWqdJv0K7oD6QSolnM78eSgczI2Q) and `?trip::sda`.

```{r}
locs$keep <-locs %>%
  trip(c("datetime","deploy_id"),correct_all=FALSE) %>%
  sda(smax=7.2) 

locs <- filter(locs, keep)
```

[ARG: I moved the subsequent three sections (Header 3) so that rather than being subsequent to the data projection steps, they come before. Devin to confirm if feasible or if need to come after.]{.underline}

### Get number of locations per animal

```{r}
anim_locs <- st_drop_geometry(locs) %>% select(deploy_id) %>% 
  group_by(deploy_id) %>% summarize(num_locs=n())
```

### Remove PTTs with \< a certain number of locations:

In this case it is tags with less than 20 locations, but that can be adjusted here.

```{r}
anim_locs <- filter(anim_locs, num_locs>20)
locs <- filter(locs, deploy_id %in% anim_locs$deploy_id)

```


### Project data for CTCRW modeling

[ARG: Devin to review the subsequent code for finding the appropriate projection (for area of interest)]{.underline}

Use `crsuggest` package to determine top projection. EPSG 6637 is the top projection for Guam.

```{r}
prj <- as.numeric(suggest_crs(locs)$crs_code[[1]])
locs <- locs %>% st_transform(prj)
```

### Obtain a land polygon for mapping

(Optional) Here we'll use the `pic_maps` package to get a spatial polygon for the coastline. You'll need to download the OSM data the first time you use it. So run the code below if this is the first time using the `picMaps` package. The `set_data_storage()` function will create a directory where the worldwide Open Street Maps (OSM) data will be downloaded. You can specify this directory to be wherever you want that has write permissions. The directory specified below is the default.

```{r, eval=FALSE}
# set_data_storage(path = "~/.picmaps_data")
# osm_download()
```

To get a land polygon for our location data we create a bounding box and buffer it by 150km (150,000m), turn it in to a spatial object and `osm_coast` will obtain all land polygons that intersect that buffer. Setting `keep=0.5` in the `osm_coast()` will retain about 50% of the raw coastline data which reduces the resolution some. The land polygons will be of the same projection as the `locs` data.

```{r, output=FALSE}
land <- locs %>% st_bbox() %>% st_as_sfc() %>% 
  st_buffer(150000) %>% osm_coast(union=TRUE, keep=0.5)
```

Alternatively, if you have a specific .shp file you would like to use instead, the following code will read in the shape file.

```{r, eval=FALSE}
# land <- read_sf("raw_data/marianas/marianas.shp")
```

### Create visibility graph for rerouting

Here a visability graph is created for routing predicted or simulated track locations around land polygons

```{r}
vis_graph <- pathroutr::prt_visgraph(land, centroids=TRUE)
```

### Save output for continuing later
If you would would like to pause here and continue working later you should save the products so you don't have to repeat these steps.
```{r}
dir.create("processed_data")
save(locs, land, vis_graph, packages, file="processed_data/clean_data.RData")
```
Otherwise, you can continue with step 2 without saving.

## Continue data analysis

Continue into step 2 directly. If R is closed prior to Step 2, Step 1 will need to be run again.
